{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the difference between precision and recall?\n",
    "\n",
    "#### **Precision**:\n",
    "- Precision measures how many of the predicted **positive** cases are actually positive.\n",
    "- **Formula**:  \n",
    "  \\[\n",
    "  \\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "  \\]\n",
    "- It focuses on **reducing false positives**.\n",
    "- **Example**:  \n",
    "  If a spam filter identifies 100 emails as spam, and 90 are truly spam, the precision is \\(90/100 = 0.9\\) or 90%.\n",
    "\n",
    "#### **Recall**:\n",
    "- Recall measures how many of the actual **positive** cases were correctly identified by the model.\n",
    "- **Formula**:  \n",
    "  \\[\n",
    "  \\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Negatives (FN)}}\n",
    "  \\]\n",
    "- It focuses on **reducing false negatives**.\n",
    "- **Example**:  \n",
    "  If there are 100 spam emails, and the model catches 90 of them, the recall is \\(90/100 = 0.9\\) or 90%.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Difference**:\n",
    "| Aspect       | Precision                              | Recall                                 |\n",
    "|--------------|----------------------------------------|----------------------------------------|\n",
    "| **Focus**    | Correctness of **positive predictions** | Completeness of identifying positives |\n",
    "| **Goal**     | Reduce **false positives**             | Reduce **false negatives**            |\n",
    "| **Use Case** | When false positives are costly (e.g., email spam detection). | When false negatives are costly (e.g., disease detection). |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What is cross-validation, and why is it important in binary classification?\n",
    "\n",
    "#### **Cross-Validation**:\n",
    "- Cross-validation is a statistical technique used to evaluate the performance of a machine learning model by dividing the data into **training** and **testing** subsets multiple times.\n",
    "\n",
    "#### **Common Types**:\n",
    "1. **K-Fold Cross-Validation**:  \n",
    "   Divides the dataset into \\( k \\) subsets (folds). The model is trained on \\( k-1 \\) folds and tested on the remaining fold, repeated \\( k \\) times.\n",
    "   \n",
    "2. **Stratified K-Fold Cross-Validation**:  \n",
    "   Ensures that each fold has a proportional representation of each class (important for imbalanced datasets).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Why is it important in binary classification?**\n",
    "1. **Reduces Overfitting**:  \n",
    "   Cross-validation provides a more reliable estimate of model performance by testing the model on unseen data in multiple splits, preventing overfitting to a specific train-test split.\n",
    "\n",
    "2. **Reliable Performance Metrics**:  \n",
    "   By averaging metrics across folds (e.g., accuracy, precision, recall, F1-score), you get a robust estimate of how the model performs on unseen data.\n",
    "\n",
    "3. **Handles Imbalanced Datasets**:  \n",
    "   Stratified cross-validation ensures class proportions are maintained in each fold, leading to a fair evaluation.\n",
    "\n",
    "4. **Utilizes Data Efficiently**:  \n",
    "   Cross-validation uses the entire dataset for training and testing, ensuring no data is wasted.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Example**:\n",
    "In a binary classification problem (e.g., spam detection), cross-validation ensures the model is tested on different subsets of data. This prevents biases and ensures that performance metrics reflect the modelâ€™s ability to generalize to unseen data, which is crucial for imbalanced datasets or limited data availability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
