{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Questions\n",
    "# Q1.\n",
    "\n",
    "## Common Hyperparameters of Decision Tree Models\n",
    "\n",
    "Decision trees are powerful models in machine learning, and their performance can be significantly influenced by the choice of hyperparameters. Below are some of the most common hyperparameters and their impact on the model's performance:\n",
    "\n",
    "### Common Hyperparameters\n",
    "\n",
    "- **`max_depth`**  \n",
    "  Limits the depth of the tree. Restricting the depth prevents overfitting by simplifying the model, but it may lead to underfitting if set too low.\n",
    "\n",
    "- **`min_samples_split`**  \n",
    "  Minimum number of samples required to split an internal node. Higher values can prevent overfitting by stopping splits with small data subsets.\n",
    "\n",
    "- **`min_samples_leaf`**  \n",
    "  Minimum number of samples required to be in a leaf node. Larger values create more balanced trees and help reduce overfitting.\n",
    "\n",
    "- **`criterion`**  \n",
    "  Determines the function used to measure the quality of splits (e.g., `gini` for Gini Impurity or `entropy` for Information Gain). It affects the way the tree decides to split.\n",
    "\n",
    "- **`max_features`**  \n",
    "  Limits the number of features considered for the best split. Reducing features can decrease overfitting but may lead to a less expressive model.\n",
    "\n",
    "- **`max_leaf_nodes`**  \n",
    "  Limits the number of leaf nodes in the tree. This helps control the complexity and reduce overfitting.\n",
    "\n",
    "- **`splitter`**  \n",
    "  Decides how the splits are made at each node (`best` vs. `random`). Using `random` may make the model less accurate but faster to train.\n",
    "\n",
    "### Effect on Model's Performance\n",
    "\n",
    "- **Overfitting vs. Underfitting**  \n",
    "  Increasing `max_depth` or lowering `min_samples_split` allows for deeper and more complex trees, which can capture more intricate patterns but risks overfitting.  \n",
    "  Using constraints like `max_leaf_nodes`, `min_samples_leaf`, or a low `max_depth` reduces overfitting but might result in underfitting if the tree is too simple.\n",
    "\n",
    "- **Choice of Criterion**  \n",
    "  The choice of `criterion` affects how the splits are evaluated, potentially impacting accuracy based on the dataset's structure.\n",
    "\n",
    "---\n",
    "# Q2.\n",
    "## Label Encoding vs. One-Hot Encoding\n",
    "\n",
    "Both **Label Encoding** and **One-Hot Encoding** are techniques used to convert categorical data into numerical form. However, each technique has its own use case and potential downsides.\n",
    "\n",
    "### Comparison Table\n",
    "\n",
    "| **Aspect**             | **Label Encoding**                                          | **One-Hot Encoding**                                      |\n",
    "|------------------------|-------------------------------------------------------------|-----------------------------------------------------------|\n",
    "| **Definition**         | Converts categorical labels into integer values.            | Converts each category into a binary vector with one active bit. |\n",
    "| **Output Format**      | Integer values (e.g., [0, 1, 2, ...]).                      | Binary vectors (e.g., [1, 0, 0], [0, 1, 0]).               |\n",
    "| **Use Case**           | Suitable for ordinal data where order matters.              | Used for nominal data where no order exists.               |\n",
    "| **Disadvantage**       | May introduce unintended ordinal relationships.             | Can create a large number of columns for high cardinality.  |\n",
    "| **Implementation**     | `LabelEncoder` from sklearn.                                | `OneHotEncoder` or `pandas.get_dummies()`.                  |\n",
    "\n",
    "### Example\n",
    "\n",
    "#### Label Encoding\n",
    "\n",
    "Given categories: `[\"Red\", \"Blue\", \"Green\"]`  \n",
    "Encoded as: `[0, 1, 2]`\n",
    "\n",
    "#### One-Hot Encoding\n",
    "\n",
    "Given categories: `[\"Red\", \"Blue\", \"Green\"]`  \n",
    "Encoded as:\n",
    "\n",
    "- **Red**: `[1, 0, 0]`  \n",
    "- **Blue**: `[0, 1, 0]`  \n",
    "- **Green**: `[0, 0, 1]`\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "- **Label Encoding** is suitable when the feature has a natural order (e.g., `\"Low\"`, `\"Medium\"`, `\"High\"`).\n",
    "- **One-Hot Encoding** is ideal for nominal features with no inherent order (e.g., `\"Dog\"`, `\"Cat\"`, `\"Bird\"`).\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
