{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499f17f8",
   "metadata": {},
   "source": [
    "## 1. What is Normalization & Standardization and how is it helpful?\n",
    "\n",
    "### ✅ Normalization:\n",
    "- **Definition:** Rescaling data to a fixed range, typically **[0, 1]**.\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "  $$\n",
    "- **Use case:** When **features have different scales** and algorithms are **distance-based** (e.g., KNN, K-Means, Neural Networks).\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Standardization (Z-score normalization):\n",
    "- **Definition:** Rescaling data to have **mean = 0** and **standard deviation = 1**.\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  X_{std} = \\frac{X - \\mu}{\\sigma}\n",
    "  $$\n",
    "  where $ \\mu $ = mean and $ \\sigma $ = standard deviation.\n",
    "\n",
    "- **Use case:** Preferred when **data is normally distributed**; used in **linear regression, PCA, logistic regression**, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Why it’s helpful:\n",
    "- Ensures **fair contribution** from each feature.\n",
    "- **Speeds up convergence** in gradient-based algorithms.\n",
    "- Prevents **bias toward features with larger scales**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. What techniques can be used to address multicollinearity in Multiple Linear Regression?\n",
    "\n",
    "> **Multicollinearity** = when **independent variables are highly correlated**, leading to **unstable coefficients**.\n",
    "\n",
    "### 🛠️ Techniques to address it:\n",
    "\n",
    "1. **Remove one of the correlated features:**\n",
    "   - Drop one of the highly correlated variables.\n",
    "\n",
    "2. **Variance Inflation Factor (VIF):**\n",
    "   - Calculate VIF for each feature.\n",
    "   - If **VIF > 5 or 10**, consider removing that feature.\n",
    "\n",
    "3. **Principal Component Analysis (PCA):**\n",
    "   - Transform correlated features into **uncorrelated principal components**.\n",
    "\n",
    "4. **Ridge Regression (L2 regularization):**\n",
    "   - Adds penalty to large coefficients.\n",
    "   - Reduces model complexity and multicollinearity effects.\n",
    "\n",
    "5. **Combine correlated features:**\n",
    "   - Create a new feature as a combination (e.g., average) of the correlated ones.\n",
    "\n",
    "6. **Domain knowledge:**\n",
    "   - Use subject matter expertise to decide which variable is more meaningful.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
