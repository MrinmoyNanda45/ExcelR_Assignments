{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: Base Model vs. Tuned Model\n",
    "\n",
    "### 1. **Overview of the Models**\n",
    "- **Base Model:** Trained using default hyperparameters (e.g., 64 neurons, ReLU activation, dropout rate of 0.2, learning rate of 0.001, batch size of 32, and 50 epochs).\n",
    "- **Tuned Model:** Trained using hyperparameter tuning (via Random Search), leading to an optimal set of parameters for better performance.\n",
    "\n",
    "### 2. **Model Performance Metrics**\n",
    "\n",
    "| Metric      | Base Model          | Tuned Model         |\n",
    "|-------------|---------------------|---------------------|\n",
    "| Accuracy    | 0.88575             | 0.90025             |\n",
    "| Precision   | 0.8899              | 0.9021              |\n",
    "| Recall      | 0.88575             | 0.90025             |\n",
    "| F1-Score    | 0.8864              | 0.9002              |\n",
    "\n",
    "### 3. **Discussion of Performance Differences**\n",
    "\n",
    "#### **Accuracy**\n",
    "- The **Tuned Model** outperforms the **Base Model** with an accuracy of **0.90025**, compared to the **Base Model's 0.88575**.\n",
    "- This indicates that the tuned model is better at classifying the instances correctly across all classes.\n",
    "\n",
    "#### **Precision**\n",
    "- The **Tuned Model** achieves a precision of **0.9021**, slightly higher than the **Base Model's precision** of **0.8899**.\n",
    "- A higher precision implies that the tuned model makes fewer false positive errors (i.e., fewer incorrect positive classifications).\n",
    "\n",
    "#### **Recall**\n",
    "- The **Recall** is also improved in the **Tuned Model** (**0.90025**) compared to the **Base Model** (**0.88575**).\n",
    "- This suggests that the tuned model is better at identifying the positive class and minimizing false negatives.\n",
    "\n",
    "#### **F1-Score**\n",
    "- The **F1-Score** of the **Tuned Model** (**0.9002**) is higher than the **Base Model's F1-Score** (**0.8864**).\n",
    "- Since F1-Score is the harmonic mean of precision and recall, the improvement reflects a balanced increase in both precision and recall, making the tuned model more robust in performance.\n",
    "\n",
    "### 4. **Impact of Hyperparameter Tuning**\n",
    "- **Tuning the hyperparameters** (such as the number of neurons, batch size, dropout rate, etc.) led to a more optimal model that is better at **handling the complexity of the data**, resulting in **better generalization** and **reduced overfitting**.\n",
    "- The **increase in accuracy** and other metrics suggests that the hyperparameter tuning allowed the model to find a more favorable configuration, capturing patterns in the data more effectively.\n",
    "  \n",
    "### 5. **Conclusion**\n",
    "The **Tuned Model** consistently outperforms the **Base Model** across all metrics, demonstrating the significant impact of hyperparameter tuning. The improvements in **accuracy**, **precision**, **recall**, and **F1-score** highlight the value of investing in hyperparameter optimization to achieve better model performance.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
